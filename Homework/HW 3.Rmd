---
title: "QFin Computing Group 2 HW 3"
output: html_notebook
---

48-55, 62-71
```{r}
cat("a) normalized number means that the digit before the seperator is not 0
      b..base, p..precision
      based on what we learned there are 2*(b-1)*b^(p-1)*(e_max-e_min+1)+1 different non-zero normalized numbers.")

cat("b) b..base, p..precision
      The largest normalized positive number would be (1-b^(-p))*(b^(e_max+1))
      The smallest normalized negative number would be 1*b^e_min.")
```

49.
```{r}
cat("Smallest values for p and emin and largest for emax?
    P(recision) has to be at least 6, as the first number 2365,27 is six digits long.
    emin has to be -7 in order to represent 0.0000512. emax has to be 3 in order to represent 2365")
```

50.
```{r}
x = 1.23456
y = 1.23579

cat("y-x is", y-x)
cat("a) y-x contain 3 significant digits")
cat("b) x=",x, "y=",y, "y-x=",y-x,"To represent x and y in normalized form we need emax to be at least 0. For y-x we need emin to be at least -3. Therefore the minimum exponent range we need is 3.")
```

51.
```{r}
cat("Largest positve non-zero number: 1+2^127 = ", 1+2^127)
cat("Smallest positive non-zero number: +2^-24*2^-127 = ",+2^-24*2^-127)
```

52.
```{r}
#smallest significant digit: b^(1-p) 
#smallest step size: b^e_min  
#minimum spacing between x and y: b^(e_min + 1 - p)

#largest step size: b^e_max 
#maximum spacing between x and y: b^(e_max + 1 - p)
```


53. 
```{r}
abs(3* (4/3-1) -1 )

# the idea is that 4/3 gets cut off after 16 decimals. therefore the absolute value results in not being 0, but the smallest possible number. this works in IEEE 754 double precision 
```

54.
```{r}
x <- c(0,7,8)
x[0.9999999999999999]   # the maximum precision we can expect for floating point computations is 16 decimal digits after the comma. 
x[0.99999999999999999]    # therefore r rounds this to 1
```

55.
```{r}
options(digits=22)

ex55a <- function(x) {
  x <- 1
  while(x + 1 != 1)
    x <- x/2
  return(x*2)
}

ex55a()

1+ex55a()

ex55b <- function(x) {
  x <- 1
  while(1 - x != 1)
    x <- x/2
  return(x*2)
}

ex55b()
1-ex55b() #2^(-53)

```


62.
```{r}
x = 2^1023 * (2-2^(-52))
y = 2^(-1022)

x
y

x^2
y^2

#If x and y can be represented without error, this does not imply that x^2 and y^2 also can be represented without error.

#(x+y)(x-y) will be more accurate than x^2-y^2 because x and y being representable implies that (x+y) and (x-y) can also be represented, while this is not the case for x^2 and y^2, per above. 

x = 2^500
y = -2^5

(x^2-y^2)
(x+y) * (x-y)
(x^2-y^2) - (x+y) * (x-y)

#Intuitively, we would expect that the error would be most different for x and y values that are very far apart, though can't find an example to show this.
```


63.
```{r}
a = 1
b = 2^(-53)
c = 2^(-54)

(a+b)+c == a+(b+c)
```

64.
```{r}
log(exp(5)) 
exp(log(5))

log(exp(5)) == exp(log(5)) 

log(exp(5)) - exp(log(5))
```
The relationship does hold approximately, as the error is very small relative to x. For most practical purposes, the relationship can be considered true in R.


65.
```{r}
e_limit = function(n) {
  return((1+(1/n))^n)
}

#Estimations for e
e_limit(10^seq(1,20))

#Error terms for all k=1,...,20
abs(e_limit(10^seq(1,20)) - exp(1))
```
The error is minimized at approximately n=10^8. For n larger than that, the error increases and converges to exp(1)-1 at approximately n=10^16. Starting at this n value, the 1/n term becomes smaller than 2^(-52), and modulo rounding errors are introduced. As a result of this imprecision, the estimate for e has very large error values for n >= 10^15 


66.
```{r}
e_approx = function(n, type){
  if (type == "first"){
    value = (1+(1/n))^n
    return(value)
  }
  else if (type == "second") {
    value = (1+n)^(1/n) #note that n=1/h
    return(value)
  }
  else {
    return("Error")
  }
}

exp(1)

for (i in seq(1:25)){
  print(e_approx(10^i, "first"))
}

for (i in seq(1:25)){
  print(e_approx(10^(-i), "second"))
}

#Using either method, e can be approximated to 6 decimal places (2.718282).
```

67.
```{r}
prime_check = function(n) {
  ((factorial(n-1)+1) %% n == 0)
}

for (i in 2:25) {
  print(paste(i, prime_check(i)))
}
```
This approach fails starting at n=20, because 19! is too large to be stored accurately in double precision floating point systems. This is not a good way for a computer to test for primality. 

68. 
```{r}
cat("
a)\n
Limit of a rational function where both numerator and denominator tends to 0 according to l`HÃ´pital's rule is the limit of the rational function composed of the derivative of the numerator and the derivative of the denominator. \n
It means that the limit of (e^x-1)/x is the limit of e^x/1=e^x, which is exactly 1 if x tends to 0. \n
")

cat("
b)\n
k \t x \t emp. appr. limit \n")
for(k in 1:15){
  x=10^-k
  cat(k, "\t", x ,"\t \t")
  cat((exp(x)-1)/x, "\n")
}
cat("
The results do not coincide with theoretical expectations. \n
For small k it can be explained by representation error in the calculation of exp(x), for large k this is because of representation errors in 1/x. \n
")

cat("

c)\n
k \t x \t emp. appr. limit \n")
for(k in 1:15){
  x=10^-k
  cat(k, "\t", x ,"\t \t")
  cat((exp(x)-1)/log(exp(x)), "\n")
}
cat("
The results again do not agree with theoretical expectation, but here only for small k. \n
The representation error in log(exp(x)) offsets the representation error in 1/x for large k.\n
")
```

69.
```{r}
approx_deriv=function(f, x){
  h=1/10^10
  return((f(x+h)-f(x))/h)
}
approx_deriv(function(x) x^2, 1)
approx_deriv(function(x) x^2, 2)
approx_deriv(function(x) x^2, -1)
approx_deriv(function(x) x^2, 0)

f=function(x) tan(x)
approx_deriv(f, 1)
1+tan(1)^2
cat("The program works rather well, its output differs from the actuall value only in the 6th significant digit, and only marginally so. \n  
")
```

70.
```{r}
cat("
Substracting numbers that are approximately equal would result in something close to zero which causes floating point representation errors. \n
Alternative formula calculation includes something that is roughly 2*b which has a lower relative error.
\n
")
```

71.
```{r}
cat("
a) \n
For very big x, very small x, or x very close to +1/-1. \n
b) \n
1/(1-x)-1/(1+x) can be rewritten as 2x/(1-x^2). \n
When x is close to +/-1, (1-x^2) is bigger than (1-x) or respectively (1+x), meaning that when we divide by it we get a smaller relative representation error. \n
Also we do not have a possible error that may arise when substracting 
1/(1-x) and 1/(1+x).
\n    
")
```
