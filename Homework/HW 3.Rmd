---
title: "QFin Computing Group 2 HW 3"
output: html_notebook
---

48-55, 62-71

53. 
```{r}
abs(3* (4/3-1) -1 )

# the idea is that 4/3 gets cut off after 16 decimals. therefore the absolute value results in not being 0, but the smallest possible number. this works in IEEE 754 double precision 
```

54.
```{r}
x <- c(0,7,8)
x[0.9999999999999999]   # the maximum precision we can expect for floating point computations is 16 decimal digits after the comma. 
x[0.99999999999999999]    # therefore r rounds this to 1
```

55.
```{r}
options(digits=22)

ex55a <- function(x) {
  x <- 1
  while(x + 1 != 1)
    x <- x/2
  return(x*2)
}

ex55a()

1+ex55a()

ex55b <- function(x) {
  x <- 1
  while(1 - x != 1)
    x <- x/2
  return(x*2)
}

ex55b()
1-ex55b() #2^(-53)

```


62.
```{r}
x = 2^1023 * (2-2^(-52))
y = 2^(-1022)

x
y

x^2
y^2

#If x and y can be represented without error, this does not imply that x^2 and y^2 also can be represented without error.

#(x+y)(x-y) will be more accurate than x^2-y^2 because x and y being representable implies that (x+y) and (x-y) can also be represented, while this is not the case for x^2 and y^2, per above. 

x = 2^500
y = -2^5

(x^2-y^2)
(x+y) * (x-y)
(x^2-y^2) - (x+y) * (x-y)

#Intuitively, we would expect that the error would be most different for x and y values that are very far apart, though can't find an example to show this.
```


63.
```{r}
a = 1
b = 2^(-53)
c = 2^(-54)

(a+b)+c == a+(b+c)
```

64.
```{r}
log(exp(5)) 
exp(log(5))

log(exp(5)) == exp(log(5)) 

log(exp(5)) - exp(log(5))
```
The relationship does hold approximately, as the error is very small relative to x. For most practical purposes, the relationship can be considered true in R.


65.
```{r}
e_limit = function(n) {
  return((1+(1/n))^n)
}

#Estimations for e
e_limit(10^seq(1,20))

#Error terms for all k=1,...,20
abs(e_limit(10^seq(1,20)) - exp(1))
```
The error is minimized at approximately n=10^8. For n larger than that, the error increases and converges to exp(1)-1 at approximately n=10^16. Starting at this n value, the 1/n term becomes smaller than 2^(-52), and modulo rounding errors are introduced. As a result of this imprecision, the estimate for e has very large error values for n >= 10^15 


66.
```{r}
e_approx = function(n, type){
  if (type == "first"){
    value = (1+(1/n))^n
    return(value)
  }
  else if (type == "second") {
    value = (1+n)^(1/n) #note that n=1/h
    return(value)
  }
  else {
    return("Error")
  }
}

exp(1)

for (i in seq(1:25)){
  print(e_approx(10^i, "first"))
}

for (i in seq(1:25)){
  print(e_approx(10^(-i), "second"))
}

#Using either method, e can be approximated to 6 decimal places (2.718282).
```

67.
```{r}
prime_check = function(n) {
  ((factorial(n-1)+1) %% n == 0)
}

for (i in 2:25) {
  print(paste(i, prime_check(i)))
}
```
This approach fails starting at n=20, because 19! is too large to be stored accurately in double precision floating point systems. This is not a good way for a computer to test for primality. 

